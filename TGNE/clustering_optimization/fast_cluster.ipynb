{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pdb\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances, silhouette_samples\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import scipy.stats as st\n",
    "import scipy.spatial\n",
    "import scipy.cluster.hierarchy\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "\n",
    "import umap\n",
    "# import pymde\n",
    "\n",
    "# import torch\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "\n",
    "from Bio import SeqIO\n",
    "\n",
    "import bokeh\n",
    "from bokeh.plotting import show as show_interactive, output_file, output_notebook\n",
    "from bokeh.layouts import column, row\n",
    "from bokeh.models import (\n",
    "    CustomJS,\n",
    "    TextInput,\n",
    "    LassoSelectTool,\n",
    "    Select,\n",
    "    MultiSelect,\n",
    "    ColorBar,\n",
    "    Legend,\n",
    "    LegendItem,\n",
    "    DataTable,\n",
    "    DateFormatter,\n",
    "    TableColumn,\n",
    "    Button,\n",
    "    HTMLTemplateFormatter,\n",
    "    FactorRange,\n",
    ")\n",
    "from bokeh.events import SelectionGeometry\n",
    "from bokeh.transform import linear_cmap, jitter\n",
    "\n",
    "from matplotlib.pyplot import show as show_static\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from pynndescent import NNDescent\n",
    "\n",
    "from csv import DictWriter\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr_df = pd.read_csv('./clr_network_for_distances.csv')\n",
    "clr_df.rename(columns={'Unnamed: 0':'TTHERM_ID'}, inplace=True)\n",
    "print(clr_df.shape)\n",
    "clr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_zscore = clr_df.max(axis=None, numeric_only=True)\n",
    "max_zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_zscore = clr_df.min(axis=None, numeric_only=True)\n",
    "min_zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_arr = clr_df.loc[:,clr_df.columns[1:]].to_numpy()\n",
    "zscore_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale zscores linearly from 0 to 1\n",
    "\n",
    "\n",
    "# distance metric of 1/zscore\n",
    "\n",
    "\n",
    "# 1/zscore for everything not zero, scale 0 to 1 linearly, assign 1s to previous zeros\n",
    "inverted_zscore_arr = (max_zscore + min_zscore) - zscore_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_zscore_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_zscore_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(inverted_zscore_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(inverted_zscore_arr.shape[0] * inverted_zscore_arr.shape[1]) - np.count_nonzero(inverted_zscore_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_idxs = np.where(inverted_zscore_arr == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "nonzero_inverted_zscore_arr = copy.deepcopy(inverted_zscore_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_pair in zero_idxs:\n",
    "    nonzero_inverted_zscore_arr[idx_pair[0]][idx_pair[1]] = 1e-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(nonzero_inverted_zscore_arr == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(nonzero_inverted_zscore_arr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_row(row):\n",
    "    shuffled_row = row.values.copy()\n",
    "    np.random.shuffle(shuffled_row)\n",
    "    return pd.Series(shuffled_row, index=row.index)\n",
    "\n",
    "def shuffle_rows(df):\n",
    "    columns_to_shuffle = df.columns[1:]\n",
    "    df[columns_to_shuffle] = df[columns_to_shuffle].apply(shuffle_row, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geom_mean_expression(expression_df):\n",
    "    \"\"\"\n",
    "    \n",
    "    Function to take an expression dataframe from the microarrays and collapse it into the means of\n",
    "    all replicate chips.\n",
    "    \"\"\"\n",
    "    # C2 and S12 got removed during quality control\n",
    "    x = [\n",
    "        'Ll', \n",
    "        'Lm', \n",
    "        'Lh', \n",
    "        'S0', \n",
    "        'S3', \n",
    "        'S6', \n",
    "        'S9', \n",
    "        # 'S12', \n",
    "        'S15', \n",
    "        'S24', \n",
    "        'C0', \n",
    "        # 'C2', \n",
    "        'C4', \n",
    "        'C6', \n",
    "        'C8', \n",
    "        'C10', \n",
    "        'C12', \n",
    "        'C14', \n",
    "        'C16', \n",
    "        'C18']\n",
    "    \n",
    "    # cols = expression_df.columns[1:]\n",
    "    # x = [c for c in x if c in cols]\n",
    "    \n",
    "    condition_expr_dict = {c.split(\"_\")[0]: [] for c in expression_df.columns[1:]}\n",
    "    \n",
    "    for c in list(expression_df.columns)[1:]:\n",
    "        \n",
    "        cond = c.split('_')[0]\n",
    "        if cond in condition_expr_dict.keys():\n",
    "            expr_list = condition_expr_dict.get(cond, [])\n",
    "\n",
    "            # Need to avoid true zeros\n",
    "            expr_list.append(expression_df[c].values)\n",
    "            condition_expr_dict[cond] = expr_list\n",
    "        \n",
    "    condition_mean_dict = {c: (st.mstats.gmean(np.array(condition_expr_dict[c]) + 1, 0) - 1) for c in condition_expr_dict.keys() if c in x}\n",
    "    \n",
    "    mean_expr_df = pd.DataFrame(condition_mean_dict)\n",
    "    mean_expr_df['TTHERM_ID'] = expression_df['TTHERM_ID'].values\n",
    "    cols = list(mean_expr_df.columns)\n",
    "    reorder = cols[-1:] + cols[:-1]\n",
    "    mean_expr_df = mean_expr_df[reorder]\n",
    "    \n",
    "    return mean_expr_df\n",
    "\n",
    "def normalizer(array):\n",
    "    \"\"\"\n",
    "    Normalizes the values of an array to range from zero to one\n",
    "    \"\"\"\n",
    "    \n",
    "    a = np.array(array)\n",
    "    \n",
    "    normalized = (array - np.min(array)) / (np.max(array) - np.min(array))\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def normalize_expression_per_gene(expression_df):\n",
    "    \"\"\"\n",
    "    Function to normalize all gene expression to range from zero to one.\n",
    "    \"\"\"\n",
    "    if 'TTHERM_ID' in expression_df.columns:\n",
    "        ttids = expression_df['TTHERM_ID'].values\n",
    "        data = expression_df[list(expression_df.columns)[1:]]\n",
    "        \n",
    "        norm_expression_df = data.apply(lambda row: normalizer(row), axis=1)\n",
    "        norm_expression_df['TTHERM_ID'] = ttids\n",
    "        \n",
    "        columns = norm_expression_df.columns.tolist()\n",
    "        \n",
    "        rearrangment = columns[-1:] + columns[:-1]\n",
    "        \n",
    "        norm_expression_df = norm_expression_df[rearrangment]\n",
    "        \n",
    "    else:\n",
    "        norm_expression_df = expression_df.apply(lambda row: normalizer(row), axis=1)\n",
    "    \n",
    "    return norm_expression_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pairwise_distance_matrix(data_df, metric, n_jobs=-1, p_minkowski=1):\n",
    "\n",
    "    if metric == 'minkowski':\n",
    "        pair_dists = pairwise_distances(data_df, metric=metric, n_jobs=n_jobs, p=p_minkowski)\n",
    "    else:\n",
    "        pair_dists = pairwise_distances(data_df, metric=metric, n_jobs=n_jobs)\n",
    "    \n",
    "    return pair_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nns(data_df, nn, metric, random_state=42, n_jobs=-1, p_minkowski=1, distance_matrix=None):\n",
    "    \n",
    "    if metric == 'clr':\n",
    "        num_neighbors = NearestNeighbors(n_neighbors=nn, metric='precomputed', n_jobs=-1).fit(distance_matrix)\n",
    "        nn_dists, nn_idxs = num_neighbors.kneighbors(return_distance=True)\n",
    "        return nn_idxs, nn_dists\n",
    "\n",
    "    n_trees = min(64, 5 + int(round((data_df.shape[0]) ** 0.5 / 20.0)))\n",
    "    n_iters = max(5, int(round(np.log2(data_df.shape[0]))))\n",
    "\n",
    "    if metric == 'minkowski':\n",
    "        knn_search_index = NNDescent(\n",
    "                data_df,\n",
    "                n_neighbors=nn,\n",
    "                metric=metric,\n",
    "                metric_kwds={'p': p_minkowski},\n",
    "                random_state=random_state,\n",
    "                n_trees=n_trees,\n",
    "                n_iters=n_iters,\n",
    "                max_candidates=60,\n",
    "                # low_memory=low_memory,\n",
    "                n_jobs=n_jobs,\n",
    "                verbose=False,\n",
    "                compressed=False,\n",
    "            )\n",
    "    else:\n",
    "        knn_search_index = NNDescent(\n",
    "                    data_df,\n",
    "                    n_neighbors=nn,\n",
    "                    metric=metric,\n",
    "                    # metric_kwds=metric_kwds,\n",
    "                    random_state=random_state,\n",
    "                    n_trees=n_trees,\n",
    "                    n_iters=n_iters,\n",
    "                    max_candidates=60,\n",
    "                    # low_memory=low_memory,\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=False,\n",
    "                    compressed=False,\n",
    "                )\n",
    "    nn_idxs, nn_dists = knn_search_index.neighbor_graph\n",
    "\n",
    "    return nn_idxs, nn_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_umap_graph(data_df, nn, metric, nn_idxs, nn_dists):\n",
    "    \n",
    "    result, sigmas, rhos, dists = umap.umap_.fuzzy_simplicial_set(data_df, nn, 42, metric, knn_indices=nn_idxs, knn_dists=nn_dists, return_dists=True)\n",
    "\n",
    "    sources, targets = result.nonzero()\n",
    "    edge_list = zip(sources, targets)\n",
    "    weights = result.data\n",
    "\n",
    "    g = ig.Graph(edges=edge_list, edge_attrs={'weight': weights})\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_leiden_partition(graph, resolution_parameter, random_state=42):\n",
    "        \n",
    "        partition = la.find_partition(graph, la.CPMVertexPartition, resolution_parameter = resolution_parameter, seed=random_state, weights='weight')\n",
    "        # partition = la.find_partition(g, la.ModularityVertexPartition, seed=42, weights='weight')\n",
    "\n",
    "        leiden_modules = np.array(partition.membership)\n",
    "\n",
    "        return leiden_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_communities(parition, idx_labels):\n",
    "    communities = {}\n",
    "\n",
    "    for idx, membership in enumerate(parition):\n",
    "        if membership not in communities:\n",
    "            communities[membership] = []\n",
    "        communities[membership].append(idx_labels[idx])\n",
    "\n",
    "    return communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_silhouette_score(distance_matrix, parition):\n",
    "    return silhouette_score(distance_matrix, parition, metric='precomputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_modularity(graph, communities):\n",
    "    nx_g = nx.Graph(graph.get_edgelist())\n",
    "    return nx.community.quality.modularity(nx_g, communities, weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_parition_for_enrichment(df, parition):\n",
    "    edf = pd.DataFrame.from_dict({'TTHERM_ID': []})\n",
    "    edf['TTHERM_ID'] = df['TTHERM_ID'].values\n",
    "    edf[f'leiden_label_full'] = parition\n",
    "    return edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_file(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_enrichment(df, parition):\n",
    "    edf = format_parition_for_enrichment(df, parition)\n",
    "\n",
    "    temp_scan_file = './temp_scan_partition.csv'\n",
    "\n",
    "    temp_enrich_file = './temp_scan_enrich.csv'\n",
    "\n",
    "    edf.to_csv(temp_scan_file, index=False)\n",
    "\n",
    "    subprocess.run(['python3', './fast_enrichment_analysis.py', temp_scan_file, temp_enrich_file])\n",
    "\n",
    "    cedf = pd.read_csv(temp_enrich_file)\n",
    "    \n",
    "    remove_file(temp_scan_file)\n",
    "\n",
    "    remove_file(temp_enrich_file)\n",
    "\n",
    "    return cedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_num_clusters(parition, communities=None):\n",
    "    if communities is None:\n",
    "        return len(set(parition))\n",
    "    \n",
    "    if len(set(parition)) != len(communities):\n",
    "        raise ValueError(f'The number of clusters/modules ({len(set(parition))}) in the parition != the number of communities ({len(communities)}).')\n",
    "    \n",
    "    return len(set(parition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cluster_sizes(communities):\n",
    "    return [len(community) for community in communities.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_enriched_cluster_sizes(communities, cedf):\n",
    "    enriched_cluster_mods = set(cedf['module'].values)\n",
    "    return [len(community) for mod, community in communities.items() if mod in enriched_cluster_mods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cluster_size_mean(cluster_sizes):\n",
    "    return np.mean(cluster_sizes)\n",
    "\n",
    "def compute_cluster_size_median(cluster_sizes):\n",
    "    return np.median(cluster_sizes)\n",
    "\n",
    "def compute_cluster_size_sd(cluster_sizes):\n",
    "    return np.std(cluster_sizes)\n",
    "\n",
    "def compute_cluster_size_sd(cluster_sizes):\n",
    "    return np.std(cluster_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_num_enriched_clusters(cedf):\n",
    "    return len(set(cedf['module'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_num_enriched_cluster_genes(edf, parition):\n",
    "    total_num_genes = 0\n",
    "\n",
    "    for m in set(edf['module'].values):\n",
    "        num_genes = np.count_nonzero(parition == int(m))\n",
    "        total_num_genes += num_genes\n",
    "    \n",
    "    return total_num_genes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(csv_file_path, data_item, header):\n",
    "    # Check if the CSV file exists and write header if it doesn't\n",
    "    if not os.path.isfile(csv_file_path):\n",
    "        with open(csv_file_path, 'w', newline='') as file:\n",
    "            writer = DictWriter(file, fieldnames=header)\n",
    "            writer.writeheader()\n",
    "\n",
    "    with open(csv_file_path, 'a', newline='') as file:\n",
    "        writer = DictWriter(file, fieldnames=header)\n",
    "        writer.writerow(data_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLUSTER START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_type = 'EXP'\n",
    "num_iterations = 1\n",
    "full_filtered_df = pd.read_csv('../microarray_probe_alignment_and_filtering/allgood_filt_agg_tidy_2021aligned_qc_rma_expression_full.csv')\n",
    "full_filtered_df = full_filtered_df.rename(columns={'Unnamed: 0': 'TTHERM_ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(42)\n",
    "# X, _ = make_blobs(n_samples=10000, n_features=30, centers=350, cluster_std=1.0, random_state=42)  # Use only 2 features\n",
    "# # Convert X to a DataFrame\n",
    "# columns = ['feature' + str(i) for i in range(X.shape[1])]\n",
    "# df = pd.DataFrame(X, columns=columns)\n",
    "# raw_data = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = 47\n",
    "\n",
    "sampler = st.qmc.LatinHypercube(d=dimensions)\n",
    "# sampler = st.qmc.Sobol(d=dimensions)\n",
    "hypercube_sample = sampler.random(n=20326)\n",
    "\n",
    "hypercube_sample.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(hypercube_sample[1234], hypercube_sample[20000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cpu_cores():\n",
    "    # If you're using Linux or macOS\n",
    "    if os.name == 'posix':\n",
    "        return os.cpu_count()\n",
    "\n",
    "    # If you're using Windows\n",
    "    elif os.name == 'nt':\n",
    "        return multiprocessing.cpu_count()\n",
    "\n",
    "    # If the operating system is not recognized\n",
    "    else:\n",
    "        return \"Unable to determine the number of CPU cores.\"\n",
    "\n",
    "# Get and print the number of CPU cores\n",
    "num_cores = get_cpu_cores()\n",
    "print(f\"Number of CPU cores: {num_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def floor_half_to_even(number):\n",
    "    return number // 4 * 2\n",
    "\n",
    "num_workers = floor_half_to_even(num_cores)\n",
    "num_workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.qmc.discrepancy(hypercube_sample, workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_datetime = str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'manhattan'\n",
    "p_minkowski = 0.5\n",
    "n_jobs = -1\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = 0.035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_iterations = 100\n",
    "# partition_type = 'NC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in tqdm.tqdm(range(num_iterations)):\n",
    "# for p_minkowski in np.arange(1.1, 2.1, 0.1):\n",
    "    \n",
    "    if partition_type == 'NC':\n",
    "        full_filtered_df = shuffle_rows(full_filtered_df)\n",
    "        \n",
    "    full_filtered_norm_df = normalize_expression_per_gene(full_filtered_df)\n",
    "    \n",
    "    raw_data = full_filtered_norm_df[list(full_filtered_norm_df.columns)[1:]].values\n",
    "    # partition_type = 'TNC'\n",
    "    # raw_data = pd.DataFrame(hypercube_sample)\n",
    "\n",
    "    idx_labels = list(range(raw_data.shape[0]))\n",
    "\n",
    "\n",
    "    distance_matrix = compute_pairwise_distance_matrix(raw_data, metric, n_jobs, p_minkowski)\n",
    "    # distance_matrix = nonzero_inverted_zscore_arr\n",
    "\n",
    "    nn_idxs, nn_dists = compute_nns(raw_data, nn, metric, random_state, n_jobs, p_minkowski, distance_matrix)\n",
    "\n",
    "    nn_graph = compute_umap_graph(raw_data, nn, metric, nn_idxs, nn_dists)\n",
    "\n",
    "    parition = compute_leiden_partition(nn_graph, rp, random_state)\n",
    "\n",
    "    communities = compute_communities(parition, idx_labels)\n",
    "\n",
    "    sil_score = compute_silhouette_score(distance_matrix, parition)\n",
    "\n",
    "    modularity = compute_modularity(nn_graph, communities.values())\n",
    "\n",
    "    enrichment_df = compute_enrichment(full_filtered_norm_df, parition)\n",
    "\n",
    "    num_clusters = compute_num_clusters(parition, communities.values())\n",
    "\n",
    "    num_enriched_clusters = compute_num_enriched_clusters(enrichment_df)\n",
    "\n",
    "    num_enriched_cluster_genes = compute_num_enriched_cluster_genes(enrichment_df, parition)\n",
    "\n",
    "    cluster_sizes = compute_cluster_sizes(communities)\n",
    "\n",
    "    enriched_cluster_sizes = compute_enriched_cluster_sizes(communities, enrichment_df)\n",
    "\n",
    "    cluster_stats = {\n",
    "    'partition_type': partition_type,\n",
    "\n",
    "    'dimensionality': 'baseline',\n",
    "\n",
    "    'metric': metric,\n",
    "    # 'metric': 'clr',\n",
    "    'graph': 'umap_fuzzy_simplicial_set',\n",
    "    'nns': nn,\n",
    "\n",
    "    'clustering': 'leiden_cpm',\n",
    "    'parameter': rp,\n",
    "\n",
    "    'silhouette_score': sil_score,\n",
    "    'modularity': modularity,\n",
    "\n",
    "    'nclusters': num_clusters,\n",
    "    'mean_cluster_size': compute_cluster_size_mean(cluster_sizes),\n",
    "    'median_cluster_size': compute_cluster_size_median(cluster_sizes),\n",
    "    'sd_cluster_size': compute_cluster_size_sd(cluster_sizes),\n",
    "\n",
    "    'nenriched_clusters': num_enriched_clusters,\n",
    "    'mean_enriched_cluster_size': compute_cluster_size_mean(enriched_cluster_sizes),\n",
    "    'median_enriched_cluster_size': compute_cluster_size_median(enriched_cluster_sizes),\n",
    "    'sd_enriched_cluster_size': compute_cluster_size_sd(enriched_cluster_sizes),\n",
    "    'nenriched_cluster_genes': num_enriched_cluster_genes,\n",
    "\n",
    "    'datetime': curr_datetime\n",
    "    }\n",
    "\n",
    "    write_to_csv('./scan_stats_v1.csv', cluster_stats, list(cluster_stats.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sil_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_enriched_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_enriched_cluster_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gene_module_assignments(all_gene_labels, gene_list, parition):\n",
    "    gene_module_assignments = {}\n",
    "\n",
    "    for gene in gene_list:\n",
    "        if gene not in all_gene_labels:\n",
    "            raise ValueError(f'The gene {gene} is not in the list of all gene labels.')\n",
    "        gene_idx = all_gene_labels.index(gene)\n",
    "        module_num = parition[gene_idx]\n",
    "        if module_num not in gene_module_assignments:\n",
    "            gene_module_assignments[module_num] = []\n",
    "        gene_module_assignments[module_num].append(gene)\n",
    "\n",
    "    return gene_module_assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list_1 = [\"TTHERM_01055600\", \"TTHERM_01002870\", \"TTHERM_01002860\", \"TTHERM_00630470\", \"TTHERM_00624730\", \"TTHERM_00624720\", \"TTHERM_00527180\", \"TTHERM_00522600\", \"TTHERM_00378890\", \"TTHERM_00335830\", \"TTHERM_00221120\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list_2 = [\"TTHERM_00420610\", \"TTHERM_00410210\", \"TTHERM_00313130\", \"TTHERM_00467390\"]\n",
    "#                                                                       MAYBE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_list_3 = [\"TTHERM_01107420\", \"TTHERM_01004990\", \"TTHERM_00985020\", \"TTHERM_00899470\", \"TTHERM_00865150\", \"TTHERM_00858130\", \"TTHERM_00849480\", \"TTHERM_00829340\", \"TTHERM_00780750\", \"TTHERM_00716180\", \"TTHERM_00704030\", \"TTHERM_00691170\", \"TTHERM_00684590\", \"TTHERM_00670190\", \"TTHERM_00571880\", \"TTHERM_00561799\", \"TTHERM_00529890\", \"TTHERM_00526250\", \"TTHERM_00469140\", \"TTHERM_00455600\", \"TTHERM_00439330\", \"TTHERM_00439030\", \"TTHERM_00424700\", \"TTHERM_00316660\", \"TTHERM_00312120\", \"TTHERM_00301770\", \"TTHERM_00297130\", \"TTHERM_00292160\", \"TTHERM_00243710\", \"TTHERM_00113120\", \"TTHERM_000711791\", \"TTHERM_00069420\", \"TTHERM_00048890\", \"TTHERM_000463439\", \"TTHERM_000439109\", \"TTHERM_00037290\", \"TTHERM_000248319\", \"TTHERM_000086999\", \"TTHERM_01079170\", \"TTHERM_01005150\", \"TTHERM_00865050\", \"TTHERM_00773520\", \"TTHERM_00729230\", \"TTHERM_00704040\", \"TTHERM_00672040\", \"TTHERM_00667000\", \"TTHERM_00648920\", \"TTHERM_00614820\", \"TTHERM_00576890\", \"TTHERM_00572090\", \"TTHERM_00483610\", \"TTHERM_00446570\", \"TTHERM_00441870\", \"TTHERM_00219420\", \"TTHERM_00194810\", \"TTHERM_00161750\", \"TTHERM_00142290\", \"TTHERM_001000210\", \"TTHERM_00083540\", \"TTHERM_00058860\", \"TTHERM_00048980\", \"TTHERM_00046130\", \"TTHERM_000420919\", \"TTHERM_000383629\", \"TTHERM_00013120\", \"TTHERM_00011190\", \"TTHERM_01245640\", \"TTHERM_01197090\", \"TTHERM_01195950\", \"TTHERM_01016190\", \"TTHERM_00790790\", \"TTHERM_00585320\", \"TTHERM_00568050\", \"TTHERM_00554270\", \"TTHERM_00498190\", \"TTHERM_00487030\", \"TTHERM_00448570\", \"TTHERM_00277550\", \"TTHERM_00242370\", \"TTHERM_00143660\", \"TTHERM_00105150\", \"TTHERM_00092850\", \"TTHERM_000011759\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gene_module_assignments(list(full_filtered_norm_df['TTHERM_ID'].values), gene_list_1, list(parition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gene_module_assignments(list(full_filtered_norm_df['TTHERM_ID'].values), gene_list_2, list(parition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gene_module_assignments(list(full_filtered_norm_df['TTHERM_ID'].values), gene_list_3, list(parition))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
