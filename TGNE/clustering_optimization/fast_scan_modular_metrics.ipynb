{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score, pairwise_distances, silhouette_samples\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "import umap\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import subprocess\n",
    "\n",
    "from pynndescent import NNDescent\n",
    "\n",
    "from csv import DictWriter\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr_df = pd.read_csv('./clr_network_for_distances.csv')\n",
    "clr_df.rename(columns={'Unnamed: 0':'TTHERM_ID'}, inplace=True)\n",
    "print(clr_df.shape)\n",
    "clr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_zscore = clr_df.max(axis=None, numeric_only=True)\n",
    "max_zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_zscore = clr_df.min(axis=None, numeric_only=True)\n",
    "min_zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zscore_arr = clr_df.loc[:,clr_df.columns[1:]].to_numpy()\n",
    "zscore_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_zscore_arr = (max_zscore + min_zscore) - zscore_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_zscore_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_zscore_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(inverted_zscore_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(inverted_zscore_arr.shape[0] * inverted_zscore_arr.shape[1]) - np.count_nonzero(inverted_zscore_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_idxs = np.where(inverted_zscore_arr == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "nonzero_inverted_zscore_arr = copy.deepcopy(inverted_zscore_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx_pair in zero_idxs:\n",
    "    nonzero_inverted_zscore_arr[idx_pair[0]][idx_pair[1]] = 1e-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(nonzero_inverted_zscore_arr == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(nonzero_inverted_zscore_arr, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_row(row):\n",
    "    shuffled_row = row.values.copy()\n",
    "    np.random.shuffle(shuffled_row)\n",
    "    return pd.Series(shuffled_row, index=row.index)\n",
    "\n",
    "def shuffle_rows(df):\n",
    "    columns_to_shuffle = df.columns[1:]\n",
    "    df[columns_to_shuffle] = df[columns_to_shuffle].apply(shuffle_row, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_geom_mean_expression(expression_df):\n",
    "    \"\"\"\n",
    "    \n",
    "    Function to take an expression dataframe from the microarrays and collapse it into the means of\n",
    "    all replicate chips.\n",
    "    \"\"\"\n",
    "    # C2 and S12 got removed during quality control\n",
    "    x = [\n",
    "        'Ll', \n",
    "        'Lm', \n",
    "        'Lh', \n",
    "        'S0', \n",
    "        'S3', \n",
    "        'S6', \n",
    "        'S9', \n",
    "        # 'S12', \n",
    "        'S15', \n",
    "        'S24', \n",
    "        'C0', \n",
    "        # 'C2', \n",
    "        'C4', \n",
    "        'C6', \n",
    "        'C8', \n",
    "        'C10', \n",
    "        'C12', \n",
    "        'C14', \n",
    "        'C16', \n",
    "        'C18']\n",
    "    \n",
    "    # cols = expression_df.columns[1:]\n",
    "    # x = [c for c in x if c in cols]\n",
    "    \n",
    "    condition_expr_dict = {c.split(\"_\")[0]: [] for c in expression_df.columns[1:]}\n",
    "    \n",
    "    for c in list(expression_df.columns)[1:]:\n",
    "        \n",
    "        cond = c.split('_')[0]\n",
    "        if cond in condition_expr_dict.keys():\n",
    "            expr_list = condition_expr_dict.get(cond, [])\n",
    "\n",
    "            # Need to avoid true zeros\n",
    "            expr_list.append(expression_df[c].values)\n",
    "            condition_expr_dict[cond] = expr_list\n",
    "        \n",
    "    condition_mean_dict = {c: (st.mstats.gmean(np.array(condition_expr_dict[c]) + 1, 0) - 1) for c in condition_expr_dict.keys() if c in x}\n",
    "    \n",
    "    mean_expr_df = pd.DataFrame(condition_mean_dict)\n",
    "    mean_expr_df['TTHERM_ID'] = expression_df['TTHERM_ID'].values\n",
    "    cols = list(mean_expr_df.columns)\n",
    "    reorder = cols[-1:] + cols[:-1]\n",
    "    mean_expr_df = mean_expr_df[reorder]\n",
    "    \n",
    "    return mean_expr_df\n",
    "\n",
    "def normalizer(array):\n",
    "    \"\"\"\n",
    "    Normalizes the values of an array to range from zero to one\n",
    "    \"\"\"\n",
    "    \n",
    "    a = np.array(array)\n",
    "    \n",
    "    normalized = (array - np.min(array)) / (np.max(array) - np.min(array))\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def normalize_expression_per_gene(expression_df):\n",
    "    \"\"\"\n",
    "    Function to normalize all gene expression to range from zero to one.\n",
    "    \"\"\"\n",
    "    if 'TTHERM_ID' in expression_df.columns:\n",
    "        ttids = expression_df['TTHERM_ID'].values\n",
    "        data = expression_df[list(expression_df.columns)[1:]]\n",
    "        \n",
    "        norm_expression_df = data.apply(lambda row: normalizer(row), axis=1)\n",
    "        norm_expression_df['TTHERM_ID'] = ttids\n",
    "        \n",
    "        columns = norm_expression_df.columns.tolist()\n",
    "        \n",
    "        rearrangment = columns[-1:] + columns[:-1]\n",
    "        \n",
    "        norm_expression_df = norm_expression_df[rearrangment]\n",
    "        \n",
    "    else:\n",
    "        norm_expression_df = expression_df.apply(lambda row: normalizer(row), axis=1)\n",
    "    \n",
    "    return norm_expression_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_type = 'EXP'\n",
    "full_filtered_df = pd.read_csv('../microarray_probe_alignment_and_filtering/allgood_filt_agg_tidy_2021aligned_qc_rma_expression_full.csv')\n",
    "full_filtered_df = full_filtered_df.rename(columns={'Unnamed: 0': 'TTHERM_ID'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_filtered_df = shuffle_rows(full_filtered_df)\n",
    "# partition_type = 'NC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_filtered_norm_df = normalize_expression_per_gene(full_filtered_df)\n",
    "raw_data = full_filtered_norm_df[list(full_filtered_norm_df.columns)[1:]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pairwise_distance_matrix(data_df, metric, n_jobs=-1, p_minkowski=1):\n",
    "\n",
    "    if metric == 'minkowski':\n",
    "        pair_dists = pairwise_distances(data_df, metric=metric, n_jobs=n_jobs, p=p_minkowski)\n",
    "    else:\n",
    "        pair_dists = pairwise_distances(data_df, metric=metric, n_jobs=n_jobs)\n",
    "    \n",
    "    return pair_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nns(data_df, nn, metric, random_state=42, n_jobs=-1, p_minkowski=1, distance_matrix=None):\n",
    "    \n",
    "    # if metric == 'clr':\n",
    "    num_neighbors = NearestNeighbors(n_neighbors=nn, metric='precomputed', n_jobs=-1).fit(distance_matrix)\n",
    "    nn_dists, nn_idxs = num_neighbors.kneighbors(return_distance=True)\n",
    "    return nn_idxs, nn_dists\n",
    "\n",
    "    # n_trees = min(64, 5 + int(round((data_df.shape[0]) ** 0.5 / 20.0)))\n",
    "    # n_iters = max(5, int(round(np.log2(data_df.shape[0]))))\n",
    "\n",
    "    # if metric == 'minkowski':\n",
    "    #     knn_search_index = NNDescent(\n",
    "    #             data_df,\n",
    "    #             n_neighbors=nn,\n",
    "    #             metric=metric,\n",
    "    #             metric_kwds={'p': p_minkowski},\n",
    "    #             random_state=random_state,\n",
    "    #             n_trees=n_trees,\n",
    "    #             n_iters=n_iters,\n",
    "    #             max_candidates=60,\n",
    "    #             # low_memory=low_memory,\n",
    "    #             n_jobs=n_jobs,\n",
    "    #             verbose=False,\n",
    "    #             compressed=False,\n",
    "    #         )\n",
    "    # else:\n",
    "    #     knn_search_index = NNDescent(\n",
    "    #                 data_df,\n",
    "    #                 n_neighbors=nn,\n",
    "    #                 metric=metric,\n",
    "    #                 # metric_kwds=metric_kwds,\n",
    "    #                 random_state=random_state,\n",
    "    #                 n_trees=n_trees,\n",
    "    #                 n_iters=n_iters,\n",
    "    #                 max_candidates=60,\n",
    "    #                 # low_memory=low_memory,\n",
    "    #                 n_jobs=n_jobs,\n",
    "    #                 verbose=False,\n",
    "    #                 compressed=False,\n",
    "    #             )\n",
    "    # nn_idxs, nn_dists = knn_search_index.neighbor_graph\n",
    "\n",
    "    # return nn_idxs, nn_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_umap_graph(data_df, nn, metric, nn_idxs, nn_dists):\n",
    "    \n",
    "    result, sigmas, rhos, dists = umap.umap_.fuzzy_simplicial_set(data_df, nn, 42, metric, knn_indices=nn_idxs, knn_dists=nn_dists, return_dists=True)\n",
    "\n",
    "    sources, targets = result.nonzero()\n",
    "    edge_list = zip(sources, targets)\n",
    "    weights = result.data\n",
    "\n",
    "    g = ig.Graph(edges=edge_list, edge_attrs={'weight': weights})\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_leiden_partition(graph, resolution_parameter, random_state=42):\n",
    "        \n",
    "        partition = la.find_partition(graph, la.CPMVertexPartition, resolution_parameter = resolution_parameter, seed=random_state, weights='weight')\n",
    "        # partition = la.find_partition(g, la.ModularityVertexPartition, seed=42, weights='weight')\n",
    "\n",
    "        leiden_modules = np.array(partition.membership)\n",
    "\n",
    "        return leiden_modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_communities(parition, idx_labels):\n",
    "    communities = {}\n",
    "\n",
    "    for idx, membership in enumerate(parition):\n",
    "        if membership not in communities:\n",
    "            communities[membership] = []\n",
    "        communities[membership].append(idx_labels[idx])\n",
    "\n",
    "    return communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_silhouette_score(distance_matrix, parition):\n",
    "    return silhouette_score(distance_matrix, parition, metric='precomputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_modularity(graph, communities):\n",
    "    nx_g = nx.Graph(graph.get_edgelist())\n",
    "    return nx.community.quality.modularity(nx_g, communities, weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_parition_for_enrichment(df, parition):\n",
    "    edf = pd.DataFrame.from_dict({'TTHERM_ID': []})\n",
    "    edf['TTHERM_ID'] = df['TTHERM_ID'].values\n",
    "    edf[f'leiden_label_full'] = parition\n",
    "    return edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_file(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_enrichment(df, parition):\n",
    "    edf = format_parition_for_enrichment(df, parition)\n",
    "\n",
    "    temp_scan_file = './temp_scan_partition.csv'\n",
    "\n",
    "    temp_enrich_file = './temp_scan_enrich.csv'\n",
    "\n",
    "    edf.to_csv(temp_scan_file, index=False)\n",
    "\n",
    "    subprocess.run(['python3', './fast_enrichment_analysis.py', temp_scan_file, temp_enrich_file])\n",
    "\n",
    "    cedf = pd.read_csv(temp_enrich_file)\n",
    "    \n",
    "    remove_file(temp_scan_file)\n",
    "\n",
    "    remove_file(temp_enrich_file)\n",
    "\n",
    "    return cedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_num_clusters(parition, communities=None):\n",
    "    if communities is None:\n",
    "        return len(set(parition))\n",
    "    \n",
    "    if len(set(parition)) != len(communities):\n",
    "        raise ValueError(f'The number of clusters/modules ({len(set(parition))}) in the parition != the number of communities ({len(communities)}).')\n",
    "    \n",
    "    return len(set(parition))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cluster_sizes(communities):\n",
    "    return [len(community) for community in communities.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_enriched_cluster_sizes(communities, cedf):\n",
    "    enriched_cluster_mods = set(cedf['module'].values)\n",
    "    return [len(community) for mod, community in communities.items() if mod in enriched_cluster_mods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cluster_size_mean(cluster_sizes):\n",
    "    return np.mean(cluster_sizes)\n",
    "\n",
    "def compute_cluster_size_median(cluster_sizes):\n",
    "    return np.median(cluster_sizes)\n",
    "\n",
    "def compute_cluster_size_sd(cluster_sizes):\n",
    "    return np.std(cluster_sizes)\n",
    "\n",
    "def compute_cluster_size_sd(cluster_sizes):\n",
    "    return np.std(cluster_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_num_enriched_clusters(cedf):\n",
    "    return len(set(cedf['module'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_num_enriched_cluster_genes(edf, parition):\n",
    "    total_num_genes = 0\n",
    "\n",
    "    for m in set(edf['module'].values):\n",
    "        num_genes = np.count_nonzero(parition == int(m))\n",
    "        total_num_genes += num_genes\n",
    "    \n",
    "    return total_num_genes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(csv_file_path, data_item, header):\n",
    "    # Check if the CSV file exists and write header if it doesn't\n",
    "    if not os.path.isfile(csv_file_path):\n",
    "        with open(csv_file_path, 'w', newline='') as file:\n",
    "            writer = DictWriter(file, fieldnames=header)\n",
    "            writer.writeheader()\n",
    "\n",
    "    with open(csv_file_path, 'a', newline='') as file:\n",
    "        writer = DictWriter(file, fieldnames=header)\n",
    "        writer.writerow(data_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCAN START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_datetime = str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_labels = list(range(raw_data.shape[0]))\n",
    "\n",
    "p_minkowski = None\n",
    "# metrics = [f'minkowski_{str(p)}' for p in np.array([0.25, 0.5, 0.75, 1, 1.5, 2, 3, 4, 5])] + ['clr', 'manhattan', 'euclidean', 'cosine']\n",
    "metrics = ['manhattan']\n",
    "n_jobs = -1\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_nns = np.arange(2, 13, 1)\n",
    "# scan_nns = [3]\n",
    "scan_nns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_rps = np.arange(0.005, 1.1, 0.005)\n",
    "# scan_rps = np.arange(0.1, 1.1, 0.1)\n",
    "# scan_rps = [0.6]\n",
    "scan_rps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric_p in metrics:\n",
    "    metric_p_split = metric_p.split('_')\n",
    "\n",
    "    metric = metric_p\n",
    "\n",
    "    if metric_p_split[0] == 'minkowski':\n",
    "        metric = metric_p_split[0]\n",
    "        p_minkowski = float(metric_p_split[1])\n",
    "\n",
    "    print(metric_p)\n",
    "    print()\n",
    "\n",
    "    if metric != 'clr':\n",
    "        distance_matrix = compute_pairwise_distance_matrix(raw_data, metric, n_jobs, p_minkowski)\n",
    "    else:\n",
    "        distance_matrix = nonzero_inverted_zscore_arr\n",
    "\n",
    "    for idx, nn in enumerate(scan_nns):     \n",
    "        print(idx+1,'of',len(scan_nns))     \n",
    "        print('NNs: ', nn)\n",
    "\n",
    "        scan_dict[nn] = {}\n",
    "\n",
    "        nn_idxs, nn_dists = compute_nns(raw_data, nn, metric, random_state, n_jobs, p_minkowski, distance_matrix)\n",
    "        scan_dict[nn]['nn_idxs'] = nn_idxs\n",
    "        scan_dict[nn]['nn_dists'] = nn_dists\n",
    "\n",
    "        nn_graph = compute_umap_graph(raw_data, nn, metric, nn_idxs, nn_dists)\n",
    "        scan_dict[nn]['nn_graph'] = nn_graph\n",
    "\n",
    "        for rp in tqdm.tqdm(scan_rps):\n",
    "\n",
    "            scan_dict[nn][rp] = {}\n",
    "            \n",
    "            parition = compute_leiden_partition(nn_graph, rp, random_state)\n",
    "            scan_dict[nn][rp]['partition'] = parition\n",
    "\n",
    "            communities = compute_communities(parition, idx_labels)\n",
    "            scan_dict[nn][rp]['communities'] = communities\n",
    "\n",
    "            sil_score = compute_silhouette_score(distance_matrix, parition)\n",
    "            scan_dict[nn][rp]['sil_score'] = sil_score\n",
    "\n",
    "            modularity = compute_modularity(nn_graph, communities.values())\n",
    "            scan_dict[nn][rp]['modularity'] = modularity\n",
    "\n",
    "            enrichment_df = compute_enrichment(full_filtered_norm_df, parition)\n",
    "            scan_dict[nn][rp]['enrichment_df'] = enrichment_df\n",
    "\n",
    "            num_clusters = compute_num_clusters(parition, communities.values())\n",
    "            scan_dict[nn][rp]['num_clusters'] = num_clusters\n",
    "\n",
    "            num_enriched_clusters = compute_num_enriched_clusters(enrichment_df)\n",
    "            scan_dict[nn][rp]['num_enriched_clusters'] = num_enriched_clusters\n",
    "\n",
    "            num_enriched_cluster_genes = compute_num_enriched_cluster_genes(enrichment_df, parition)\n",
    "            scan_dict[nn][rp]['num_enriched_cluster_genes'] = num_enriched_cluster_genes\n",
    "\n",
    "            cluster_sizes = compute_cluster_sizes(communities)\n",
    "            scan_dict[nn][rp]['cluster_sizes'] = cluster_sizes\n",
    "\n",
    "            enriched_cluster_sizes = compute_enriched_cluster_sizes(communities, enrichment_df)\n",
    "            scan_dict[nn][rp]['enriched_cluster_sizes'] = enriched_cluster_sizes\n",
    "\n",
    "            cluster_stats = {\n",
    "            'partition_type': partition_type,\n",
    "\n",
    "            'dimensionality': 'baseline',\n",
    "\n",
    "            'metric': metric_p,\n",
    "            'graph': 'umap_fuzzy_simplicial_set',\n",
    "            'nns': nn,\n",
    "\n",
    "            'clustering': 'leiden_cpm',\n",
    "            'parameter': rp,\n",
    "\n",
    "            'silhouette_score': sil_score,\n",
    "            'modularity': modularity,\n",
    "\n",
    "            'nclusters': num_clusters,\n",
    "            'mean_cluster_size': compute_cluster_size_mean(cluster_sizes),\n",
    "            'median_cluster_size': compute_cluster_size_median(cluster_sizes),\n",
    "            'sd_cluster_size': compute_cluster_size_sd(cluster_sizes),\n",
    "\n",
    "            'nenriched_clusters': num_enriched_clusters,\n",
    "            'mean_enriched_cluster_size': compute_cluster_size_mean(enriched_cluster_sizes),\n",
    "            'median_enriched_cluster_size': compute_cluster_size_median(enriched_cluster_sizes),\n",
    "            'sd_enriched_cluster_size': compute_cluster_size_sd(enriched_cluster_sizes),\n",
    "            'nenriched_cluster_genes': num_enriched_cluster_genes,\n",
    "\n",
    "            'datetime': curr_datetime\n",
    "            }\n",
    "\n",
    "            write_to_csv('./scan_stats_sklearn_distance_v2.csv', cluster_stats, list(cluster_stats.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('./scan_stats_sklearn_distance_v2.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
